{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1 Author:\n",
    "- Student Name: **Yuxuan Wang**\n",
    "- Student ID: **210985403**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 Problem formulation\n",
    "\n",
    "**Classification Task:** Utilizing a VGG neural network, the project aims to categorize images based on whether individuals are smiling or not. The genki4k dataset, comprising a variety of facial expressions, presents a unique challenge due to the substantial variation in the appearance and intensity of smiles across different demographics. This increases the complexity of classification. Applying the VGG network specifically to the nuanced task of recognizing such subtle expressions represents an innovative approach in facial expression recognition.\n",
    "\n",
    "**Regression Task:** This aspect involves modifying the structure of the VGG network to enable it to perform regression tasks, specifically estimating the 3D posture of a person from an image. This task is technically challenging as it requires the network to not only identify facial features but also to comprehend the 3D structure of human posture. Training the network with 3D posture labels from the genki4k dataset enhances the model's capability to understand spatial information. The transformation from 2D image analysis to 3D posture estimation represents a cutting-edge challenge in the field of machine learning, particularly when training is constrained to limited posture data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Machine Learning Pipeline\n",
    "This pipeline leverages a deep neural network, specifically the VGG16 model, for image classification tasks.\n",
    "\n",
    "## Input\n",
    "The input to this pipeline is a set of images. These images are raw data that need to be preprocessed before they can be fed into the neural network for classification.\n",
    "\n",
    "## Preprocessing and Transformation\n",
    "We proform data augmentation and normalization in the transformation stage. (Details in the next section)\n",
    "\n",
    "\n",
    "## Output\n",
    "The output of this ML pipeline is the classification of the input image into one of the predefined classes or the feature of the input image, determined by the task of the neural network.\n",
    "\n",
    "## Flow of Data\n",
    "The Pipeline（flow of data） through this pipeline is sequential :\n",
    "\n",
    "1. Input images undergo preprocessing and transformation.\n",
    "2. Transformed images are then fed into the VGG16 model.\n",
    "3. The model processes these images through its layers, extracting features and performing classification or regression.\n",
    "4. For classification task the final output is the class prediction for each input image.\n",
    "5. For regression task the final output is the 3D posture estimation for each input image."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 Transformation stage\n",
    "We have data augmentation and normalization in the transformation stage.\n",
    "For training data, we have data augmentation and normalization. For validation and test data, we only have normalization.\n",
    "\n",
    "## Data Augmentation:\n",
    "The `transforms.Compose` function is used for data augmentation, includes:\n",
    "- Random horizontal flipping.\n",
    "- Random rotation by 10 degrees.\n",
    "- Color jittering for brightness, contrast, and saturation adjustments.\n",
    "- Resizing images to 128x128 pixels.\n",
    "- Converting images to grayscale and then replicating the channel to maintain a 3-channel input.\n",
    "## Normalization:\n",
    "Each color channel is normalized with a mean and standard deviation of 0.5. This standardization simplifies the optimization process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 Modelling\n",
    "\n",
    "The **VGG16** network model, as defined in the Vgg16_net class, is a convolutional neural network (CNN) known for its depth and simplicity. The VGG16 model is a popular choice for image classification tasks due to its simplicity and high accuracy.It consists of following layers.\n",
    "\n",
    "- Convolutional Layers: The model consists of five blocks of convolutional layers, each comprising 2-3 convolutional layers with ReLU activation and Batch Normalization, followed by a max-pooling layer. These layers are responsible for feature extraction from the input images.\n",
    "\n",
    "- Fully Connected Layers: After the convolutional layers, the network includes three fully connected layers. The output of the convolutional layers is flattened before being passed through these dense layers.\n",
    "\n",
    "The final layer outputs the class probabilities based on the number of classes specified (num_classes).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Methodology\n",
    "\n",
    "## Model Training and Validation\n",
    "We have these operations to promote the training process:\n",
    "\n",
    "### Loss Function:\n",
    "The Cross Entropy Loss (`nn.CrossEntropyLoss()`), which is well-suited for multi-class classification problems. This loss function combines Log Softmax and Negative Log Likelihood Loss in a single function.\n",
    "\n",
    "The Mean Squared Error (`nn.MSELoss()`), which is well-suited for regression problems. This loss function computes the mean squared error between the input and target.\n",
    "\n",
    "### Optimizer:\n",
    "The Stochastic Gradient Descent (SGD) optimizer is used (`torch.optim.SGD`). It's configured with a learning rate of 0.01, momentum of 0.8, and weight decay of 0.001. These parameters are chosen to balance the speed and stability of the learning process.\n",
    "\n",
    "### Learning Rate Scheduler:\n",
    "A learning rate scheduler (`torch.optim.lr_scheduler.StepLR`) is employed to adjust the learning rate. It reduces the learning rate by a factor of 0.5 every 10 epochs (`gamma=0.5`, `step_size=10`). This strategy helps in fine-tuning the model as training progresses.\n",
    "\n",
    "### Training Process:\n",
    "The model will be trained over 20 epochs (`num_epochs = 20`). Each epoch involves a training phase (`train(epoch)`), a validation phase and a test phase (`test()`). The training phase updates the model's weights, while the validation phase assesses its performance on unseen data.\n",
    "\n",
    "#### Training Phase (`train(epoch)`)\n",
    "- Mode Setting: At the beginning of each epoch, the model is set to training mode (`model.train()`), which enables the batch normalization and dropout layers to function in their training-specific modes.\n",
    "- Batch Processing: The training dataset is loaded in batches using train_loader. For each batch, the following steps are performed:\n",
    "- Data Preparation: Input data and target labels are loaded onto the device (e.g., `GPU` or `CPU`).\n",
    "- Gradient Initialization: Gradients are zeroed out (`optimizer.zero_grad()`) to prevent accumulation from previous forward passes.\n",
    "- Forward Pass: The model processes the input data (`model(data)`) to compute predictions.\n",
    "- Loss Computation: The loss is calculated using the Cross Entropy Loss function between the predictions and actual targets.\n",
    "- Backward Pass: Backpropagation is conducted (`loss.backward()`) to compute the gradient of the loss with respect to model parameters.\n",
    "- Parameter Update: The optimizer updates the model's weights based on the computed gradients (`optimizer.step()`).\n",
    "- Training Monitoring: For every tenth batch (or as specified), training progress is printed, showing the current loss and the percentage of data processed.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Validation Phase\n",
    "After each training epoch, the model is switched to evaluation mode (`model.eval()`), which disables specific layers like dropout for consistent behavior during inference.\n",
    "\n",
    "- Loss and Accuracy Calculation: The validation dataset is processed in a similar manner as the training data, but without updating the model’s weights. For each batch:\n",
    "Loss is calculated and accumulated.\n",
    "Predictions are compared with actual targets to compute the number of correct predictions.\n",
    "- Performance Metrics: After processing all validation batches, the average validation loss and accuracy are calculated and printed.\n",
    "Learning Rate Scheduler\n",
    "Post validation, the learning rate scheduler (`scheduler.step()`) is called to adjust the learning rate based on the predefined policy. This step is crucial for adapting the learning process and potentially improving convergence.\n",
    "\n",
    "---\n",
    "\n",
    "#### Testing Phase (`test()`)\n",
    "The testing phase, conducted in a similar manner as the validation phase, evaluates the model on a separate test set.\n",
    "\n",
    "## Performance Assessment\n",
    "The performance of the model will be evaluated using the following metrics:\n",
    "\n",
    "### Accuracy:\n",
    "This metric will assess the percentage of correct predictions. It provides a straightforward measure of how often the model is correct across all classes.\n",
    "\n",
    "### Confusion Matrix:\n",
    "The confusion matrix will be used to visualize the performance of the model in terms of the true and predicted classifications. It helps in understanding not just the errors of the model, but also the types of errors (i.e., which classes are being confused with each other)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7 Dataset\n",
    "\n",
    "## Directory Structure\n",
    "The dataset is structured into three phases: training (train), validation (val), and testing (test). For each phase in classification task, there are two subdirectories corresponding to our categories: 'smile' and 'not_smile'. And for the regression task, we have directories named with `reg_xx` for each phase. This organization facilitates the process of feeding data into our models for different purposes (training, validation, and testing).\n",
    "\n",
    "Here is the directory structure:\n",
    "```\n",
    "Dataset/\n",
    "├── train/\n",
    "│   ├── smile/\n",
    "│   └── not_smile/\n",
    "├── val/\n",
    "│   ├── smile/\n",
    "│   └── not_smile/\n",
    "├── test/\n",
    "│   ├── smile/\n",
    "│   └── not_smile/\n",
    "├── reg_train/\n",
    "├── reg_val/\n",
    "└── reg_test/\n",
    "```\n",
    "\n",
    "## Splitting and Copying\n",
    "The dataset is initially split into the aforementioned categories and phases. This is achieved through a function split_and_copy which randomly shuffles the images within each category and then divides them into training (70%), validation (20%), and testing (10%) sets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8 Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from termcolor import colored\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification Task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 拆分数据集并复制文件\n",
    "def split_and_copy(start, end, category):\n",
    "    frames = [f\"file{i:04d}.jpg\" for i in range(start, end)]\n",
    "    random.shuffle(frames)  # 随机打乱\n",
    "    train = frames[:int(len(frames) * 0.7)]\n",
    "    val = frames[int(len(frames) * 0.7):int(len(frames) * 0.9)]\n",
    "    test = frames[int(len(frames) * 0.9):]\n",
    "\n",
    "    for frame in train:\n",
    "        shutil.copy(os.path.join(base_dir, \"files\", frame),\n",
    "                    os.path.join(base_dir, \"train\", f\"{category}\", frame))\n",
    "    for frame in val:\n",
    "        shutil.copy(os.path.join(base_dir, \"files\", frame),\n",
    "                    os.path.join(base_dir, \"val\", f\"{category}\", frame))\n",
    "    for frame in test:\n",
    "        shutil.copy(os.path.join(base_dir, \"files\", frame),\n",
    "                    os.path.join(base_dir, \"test\", f\"{category}\", frame))\n",
    "\n",
    "\n",
    "# 对于每个类别执行拆分和复制\n",
    "split_and_copy(1, 2163, \"smile\")  # smile 类别\n",
    "split_and_copy(2163, 4001, \"not_smile\")  # not_smile 类别"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/smile: 1513\n",
      "train/not_smile: 1286\n",
      "val/smile: 432\n",
      "val/not_smile: 368\n",
      "test/smile: 217\n",
      "test/not_smile: 184\n"
     ]
    }
   ],
   "source": [
    "# 定义路径变量\n",
    "base_dir = \"Dataset\"\n",
    "categories = [\"smile\", \"not_smile\"]\n",
    "phases = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# 创建目录结构\n",
    "for phase in phases:\n",
    "    for category in categories:\n",
    "        os.makedirs(os.path.join(base_dir, f\"{phase}/{category}\"), exist_ok=True)\n",
    "# check the number of images in each folder\n",
    "for phase in phases:\n",
    "    for category in categories:\n",
    "        print(f\"{phase}/{category}: {len(os.listdir(os.path.join(base_dir, phase, category)))}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 定义自定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        初始化函数\n",
    "        root_dir (string): 数据集的根目录。\n",
    "        transform (callable, optional): 应用于每个样本的转换。\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        # 定义类别与标签的映射\n",
    "        class_to_label = {\"not_smile\": 0, \"smile\": 1}\n",
    "\n",
    "        # 遍历每个子目录\n",
    "        for subdir in sorted(os.listdir(root_dir)):\n",
    "            folder = os.path.join(root_dir, subdir)\n",
    "            if os.path.isdir(folder):\n",
    "                label = class_to_label[subdir]  # 根据文件夹名称获取标签\n",
    "                for file in os.listdir(folder):\n",
    "                    if file.lower().endswith(\"jpg\"):\n",
    "                        self.images.append(os.path.join(folder, file))\n",
    "                        self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# 定义图像转换\n",
    "transform_TRAIN = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 创建数据集\n",
    "train_dataset = CustomDataset(root_dir='Dataset/train', transform=transform_TRAIN)\n",
    "val_dataset = CustomDataset(root_dir='Dataset/val', transform=transform)\n",
    "test_dataset = CustomDataset(root_dir='Dataset/test', transform=transform)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# VGG16网络模型\n",
    "class Vgg16_net(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Vgg16_net, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # 第一层卷积层\n",
    "        self.layer1 = nn.Sequential(\n",
    "            # 输入3通道图像，输出64通道特征图，卷积核大小3x3，步长1，填充1\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            # 对64通道特征图进行Batch Normalization\n",
    "            nn.BatchNorm2d(64),\n",
    "            # 对64通道特征图进行ReLU激活函数\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 输入64通道特征图，输出64通道特征图，卷积核大小3x3，步长1，填充1\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            # 对64通道特征图进行Batch Normalization\n",
    "            nn.BatchNorm2d(64),\n",
    "            # 对64通道特征图进行ReLU激活函数\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # 进行2x2的最大池化操作，步长为2\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # 第二层卷积层\n",
    "        self.layer2 = nn.Sequential(\n",
    "            # 输入64通道特征图，输出128通道特征图，卷积核大小3x3，步长1，填充1\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            # 对128通道特征图进行Batch Normalization\n",
    "            nn.BatchNorm2d(128),\n",
    "            # 对128通道特征图进行ReLU激活函数\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # 输入128通道特征图，输出128通道特征图，卷积核大小3x3，步长1，填充1\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            # 对128通道特征图进行Batch Normalization\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 进行2x2的最大池化操作，步长为2\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        # 第三层卷积层\n",
    "        self.layer3 = nn.Sequential(\n",
    "            # 输入为128通道，输出为256通道，卷积核大小为33，步长为1，填充大小为1\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            # 批归一化\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            self.layer1,\n",
    "            self.layer2,\n",
    "            self.layer3,\n",
    "            self.layer4,\n",
    "            self.layer5\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * 4 * 4, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(256, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)  # 将卷积层输出的多维数据压平成一维\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           1,792\n",
      "            Conv2d-2         [-1, 64, 128, 128]           1,792\n",
      "       BatchNorm2d-3         [-1, 64, 128, 128]             128\n",
      "       BatchNorm2d-4         [-1, 64, 128, 128]             128\n",
      "              ReLU-5         [-1, 64, 128, 128]               0\n",
      "              ReLU-6         [-1, 64, 128, 128]               0\n",
      "            Conv2d-7         [-1, 64, 128, 128]          36,928\n",
      "            Conv2d-8         [-1, 64, 128, 128]          36,928\n",
      "       BatchNorm2d-9         [-1, 64, 128, 128]             128\n",
      "      BatchNorm2d-10         [-1, 64, 128, 128]             128\n",
      "             ReLU-11         [-1, 64, 128, 128]               0\n",
      "             ReLU-12         [-1, 64, 128, 128]               0\n",
      "        MaxPool2d-13           [-1, 64, 64, 64]               0\n",
      "        MaxPool2d-14           [-1, 64, 64, 64]               0\n",
      "           Conv2d-15          [-1, 128, 64, 64]          73,856\n",
      "           Conv2d-16          [-1, 128, 64, 64]          73,856\n",
      "      BatchNorm2d-17          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-18          [-1, 128, 64, 64]             256\n",
      "             ReLU-19          [-1, 128, 64, 64]               0\n",
      "             ReLU-20          [-1, 128, 64, 64]               0\n",
      "           Conv2d-21          [-1, 128, 64, 64]         147,584\n",
      "           Conv2d-22          [-1, 128, 64, 64]         147,584\n",
      "      BatchNorm2d-23          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-24          [-1, 128, 64, 64]             256\n",
      "             ReLU-25          [-1, 128, 64, 64]               0\n",
      "             ReLU-26          [-1, 128, 64, 64]               0\n",
      "        MaxPool2d-27          [-1, 128, 32, 32]               0\n",
      "        MaxPool2d-28          [-1, 128, 32, 32]               0\n",
      "           Conv2d-29          [-1, 256, 32, 32]         295,168\n",
      "           Conv2d-30          [-1, 256, 32, 32]         295,168\n",
      "      BatchNorm2d-31          [-1, 256, 32, 32]             512\n",
      "      BatchNorm2d-32          [-1, 256, 32, 32]             512\n",
      "             ReLU-33          [-1, 256, 32, 32]               0\n",
      "             ReLU-34          [-1, 256, 32, 32]               0\n",
      "           Conv2d-35          [-1, 256, 32, 32]         590,080\n",
      "           Conv2d-36          [-1, 256, 32, 32]         590,080\n",
      "      BatchNorm2d-37          [-1, 256, 32, 32]             512\n",
      "      BatchNorm2d-38          [-1, 256, 32, 32]             512\n",
      "             ReLU-39          [-1, 256, 32, 32]               0\n",
      "             ReLU-40          [-1, 256, 32, 32]               0\n",
      "           Conv2d-41          [-1, 256, 32, 32]         590,080\n",
      "           Conv2d-42          [-1, 256, 32, 32]         590,080\n",
      "      BatchNorm2d-43          [-1, 256, 32, 32]             512\n",
      "      BatchNorm2d-44          [-1, 256, 32, 32]             512\n",
      "             ReLU-45          [-1, 256, 32, 32]               0\n",
      "             ReLU-46          [-1, 256, 32, 32]               0\n",
      "        MaxPool2d-47          [-1, 256, 16, 16]               0\n",
      "        MaxPool2d-48          [-1, 256, 16, 16]               0\n",
      "           Conv2d-49          [-1, 512, 16, 16]       1,180,160\n",
      "           Conv2d-50          [-1, 512, 16, 16]       1,180,160\n",
      "      BatchNorm2d-51          [-1, 512, 16, 16]           1,024\n",
      "      BatchNorm2d-52          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-53          [-1, 512, 16, 16]               0\n",
      "             ReLU-54          [-1, 512, 16, 16]               0\n",
      "           Conv2d-55          [-1, 512, 16, 16]       2,359,808\n",
      "           Conv2d-56          [-1, 512, 16, 16]       2,359,808\n",
      "      BatchNorm2d-57          [-1, 512, 16, 16]           1,024\n",
      "      BatchNorm2d-58          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-59          [-1, 512, 16, 16]               0\n",
      "             ReLU-60          [-1, 512, 16, 16]               0\n",
      "           Conv2d-61          [-1, 512, 16, 16]       2,359,808\n",
      "           Conv2d-62          [-1, 512, 16, 16]       2,359,808\n",
      "      BatchNorm2d-63          [-1, 512, 16, 16]           1,024\n",
      "      BatchNorm2d-64          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-65          [-1, 512, 16, 16]               0\n",
      "             ReLU-66          [-1, 512, 16, 16]               0\n",
      "        MaxPool2d-67            [-1, 512, 8, 8]               0\n",
      "        MaxPool2d-68            [-1, 512, 8, 8]               0\n",
      "           Conv2d-69            [-1, 512, 8, 8]       2,359,808\n",
      "           Conv2d-70            [-1, 512, 8, 8]       2,359,808\n",
      "      BatchNorm2d-71            [-1, 512, 8, 8]           1,024\n",
      "      BatchNorm2d-72            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-73            [-1, 512, 8, 8]               0\n",
      "             ReLU-74            [-1, 512, 8, 8]               0\n",
      "           Conv2d-75            [-1, 512, 8, 8]       2,359,808\n",
      "           Conv2d-76            [-1, 512, 8, 8]       2,359,808\n",
      "      BatchNorm2d-77            [-1, 512, 8, 8]           1,024\n",
      "      BatchNorm2d-78            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-79            [-1, 512, 8, 8]               0\n",
      "             ReLU-80            [-1, 512, 8, 8]               0\n",
      "           Conv2d-81            [-1, 512, 8, 8]       2,359,808\n",
      "           Conv2d-82            [-1, 512, 8, 8]       2,359,808\n",
      "      BatchNorm2d-83            [-1, 512, 8, 8]           1,024\n",
      "      BatchNorm2d-84            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-85            [-1, 512, 8, 8]               0\n",
      "             ReLU-86            [-1, 512, 8, 8]               0\n",
      "        MaxPool2d-87            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-88            [-1, 512, 4, 4]               0\n",
      "           Linear-89                  [-1, 512]       4,194,816\n",
      "             ReLU-90                  [-1, 512]               0\n",
      "          Dropout-91                  [-1, 512]               0\n",
      "           Linear-92                  [-1, 256]         131,328\n",
      "             ReLU-93                  [-1, 256]               0\n",
      "          Dropout-94                  [-1, 256]               0\n",
      "           Linear-95                    [-1, 2]             514\n",
      "================================================================\n",
      "Total params: 33,772,930\n",
      "Trainable params: 33,772,930\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 210.14\n",
      "Params size (MB): 128.83\n",
      "Estimated Total Size (MB): 339.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 创建模型实例，对于微笑分类任务，我们只有两个类别\n",
    "model = Vgg16_net()\n",
    "# Move the model to the GPU\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, input_size=(3, 128, 128))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "loss_object = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.8, weight_decay=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5, last_epoch=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    train_loss = 0\n",
    "    print(colored(\"Train Epoch:\", \"red\"), colored(f\" {epoch} \", \"blue\"), )\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_object(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % 10 == 5:\n",
    "            print(\n",
    "                f\"[{batch_idx * len(data):5d}/{len(train_loader.dataset)} \"\n",
    "                f\"({100. * batch_idx / len(train_loader):3.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "    # 在每个 epoch 后进行验证\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += loss_object(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100. * correct / len(val_loader.dataset)\n",
    "    print(\n",
    "        colored(\"\\nValidation\", \"blue\"),\n",
    "        f'Average loss: {val_loss:.4f},'\n",
    "        f'Accuracy: {correct}/{len(val_loader.dataset)} ',\n",
    "        colored(f'({val_accuracy:.0f}%)', \"green\"), )\n",
    "\n",
    "    # 更新学习率调度器\n",
    "    print(colored(\"Learning rate:\", \"blue\"), colored(f\"{scheduler.get_last_lr()[0]:.6f}\", \"green\"))\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "# Test\n",
    "def test():\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # 在评估模式下，不计算梯度\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_object(output, target).item()  # 累积损失\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # 获取预测结果\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        colored(\"Test set\", \"yellow\"),\n",
    "        f'Average loss: {test_loss:.4f},'\n",
    "        f'Accuracy: {correct}/{len(test_loader.dataset)} ',\n",
    "        colored(f'({accuracy:.0f}%)\\n', \"green\"),\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 1 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.064262\n",
      "[  480/2799 ( 17%)]\tLoss: 0.025822\n",
      "[  800/2799 ( 28%)]\tLoss: 0.203019\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.145573\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.033916\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.032226\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.038646\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.079412\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.055829\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.3105,Accuracy: 726/800  \u001B[32m(91%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.2349,Accuracy: 366/401  \u001B[32m(91%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 1 cost 14s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 2 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.040952\n",
      "[  480/2799 ( 17%)]\tLoss: 0.010501\n",
      "[  800/2799 ( 28%)]\tLoss: 0.013165\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.032329\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.035435\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.021893\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.028379\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.046896\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.038145\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.3329,Accuracy: 729/800  \u001B[32m(91%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.2335,Accuracy: 374/401  \u001B[32m(93%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 2 cost 13s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 3 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.040500\n",
      "[  480/2799 ( 17%)]\tLoss: 0.006465\n",
      "[  800/2799 ( 28%)]\tLoss: 0.049972\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.016145\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.002393\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.229946\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.133263\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.005581\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.003769\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.3191,Accuracy: 726/800  \u001B[32m(91%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.2401,Accuracy: 370/401  \u001B[32m(92%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 3 cost 13s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 4 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.142361\n",
      "[  480/2799 ( 17%)]\tLoss: 0.012658\n",
      "[  800/2799 ( 28%)]\tLoss: 0.040631\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.011349\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.008857\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.053842\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.103033\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.007486\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.007769\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.3701,Accuracy: 717/800  \u001B[32m(90%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.2800,Accuracy: 366/401  \u001B[32m(91%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 4 cost 13s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 5 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.009177\n",
      "[  480/2799 ( 17%)]\tLoss: 0.015863\n",
      "[  800/2799 ( 28%)]\tLoss: 0.007130\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.057242\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.014048\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.014495\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.198352\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.009559\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.012401\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.3241,Accuracy: 732/800  \u001B[32m(92%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.2598,Accuracy: 368/401  \u001B[32m(92%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 5 cost 13s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 6 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.070834\n",
      "[  480/2799 ( 17%)]\tLoss: 0.073908\n",
      "[  800/2799 ( 28%)]\tLoss: 0.061460\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.012165\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.002745\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.002314\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.001808\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.002058\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.002224\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.4297,Accuracy: 720/800  \u001B[32m(90%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.3095,Accuracy: 371/401  \u001B[32m(93%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 6 cost 14s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 7 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.003405\n",
      "[  480/2799 ( 17%)]\tLoss: 0.035233\n",
      "[  800/2799 ( 28%)]\tLoss: 0.024305\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.009123\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.013550\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.006523\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.014313\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.004292\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.007108\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.4141,Accuracy: 719/800  \u001B[32m(90%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.3076,Accuracy: 368/401  \u001B[32m(92%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 7 cost 14s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 8 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.002404\n",
      "[  480/2799 ( 17%)]\tLoss: 0.006473\n",
      "[  800/2799 ( 28%)]\tLoss: 0.017994\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.066068\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.003331\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.029814\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.009890\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.010232\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.093827\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.3644,Accuracy: 734/800  \u001B[32m(92%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.2521,Accuracy: 371/401  \u001B[32m(93%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 8 cost 14s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 9 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.012853\n",
      "[  480/2799 ( 17%)]\tLoss: 0.001872\n",
      "[  800/2799 ( 28%)]\tLoss: 0.132832\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.011232\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.001432\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.009678\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.003756\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.057618\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.018941\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.3900,Accuracy: 731/800  \u001B[32m(91%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.2749,Accuracy: 367/401  \u001B[32m(92%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 9 cost 14s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 10 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.013319\n",
      "[  480/2799 ( 17%)]\tLoss: 0.103807\n",
      "[  800/2799 ( 28%)]\tLoss: 0.002778\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.065999\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.013983\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.042116\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.066867\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.000587\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.021180\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.4051,Accuracy: 738/800  \u001B[32m(92%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.2813,Accuracy: 366/401  \u001B[32m(91%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 10 cost 14s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 11 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.004502\n",
      "[  480/2799 ( 17%)]\tLoss: 0.004069\n",
      "[  800/2799 ( 28%)]\tLoss: 0.019470\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.014744\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.004550\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.003731\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.001461\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.008545\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.003025\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.4013,Accuracy: 733/800  \u001B[32m(92%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.2810,Accuracy: 367/401  \u001B[32m(92%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 11 cost 14s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 12 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.000989\n",
      "[  480/2799 ( 17%)]\tLoss: 0.004642\n",
      "[  800/2799 ( 28%)]\tLoss: 0.003260\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.060827\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.023603\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.002262\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.001470\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.002544\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.003241\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.3891,Accuracy: 735/800  \u001B[32m(92%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.2738,Accuracy: 368/401  \u001B[32m(92%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 12 cost 14s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 13 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.002002\n",
      "[  480/2799 ( 17%)]\tLoss: 0.003446\n",
      "[  800/2799 ( 28%)]\tLoss: 0.005122\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.011364\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.007547\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.001107\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.003267\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.051970\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.007800\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.4088,Accuracy: 734/800  \u001B[32m(92%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.2844,Accuracy: 366/401  \u001B[32m(91%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 13 cost 14s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 14 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.005951\n",
      "[  480/2799 ( 17%)]\tLoss: 0.000353\n",
      "[  800/2799 ( 28%)]\tLoss: 0.002256\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.010291\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.008996\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.003365\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.001325\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.005394\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.000877\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.4110,Accuracy: 732/800  \u001B[32m(92%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.2789,Accuracy: 373/401  \u001B[32m(93%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 14 cost 14s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 15 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.004492\n",
      "[  480/2799 ( 17%)]\tLoss: 0.001690\n",
      "[  800/2799 ( 28%)]\tLoss: 0.004424\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.000435\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.001793\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.005230\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.008113\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.061877\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.001912\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.4282,Accuracy: 735/800  \u001B[32m(92%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.3233,Accuracy: 370/401  \u001B[32m(92%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 15 cost 13s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 16 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.004747\n",
      "[  480/2799 ( 17%)]\tLoss: 0.036614\n",
      "[  800/2799 ( 28%)]\tLoss: 0.000796\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.005508\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.005792\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.001725\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.016156\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.001762\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.007833\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.4358,Accuracy: 736/800  \u001B[32m(92%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.3194,Accuracy: 367/401  \u001B[32m(92%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 16 cost 13s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 17 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.004641\n",
      "[  480/2799 ( 17%)]\tLoss: 0.000772\n",
      "[  800/2799 ( 28%)]\tLoss: 0.000988\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.000568\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.001317\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.001964\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.002181\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.091008\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.001699\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.5193,Accuracy: 724/800  \u001B[32m(90%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.3353,Accuracy: 373/401  \u001B[32m(93%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 17 cost 13s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 18 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.000556\n",
      "[  480/2799 ( 17%)]\tLoss: 0.003288\n",
      "[  800/2799 ( 28%)]\tLoss: 0.001675\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.001499\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.000582\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.001107\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.000074\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.000672\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.000761\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.4661,Accuracy: 735/800  \u001B[32m(92%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.3892,Accuracy: 370/401  \u001B[32m(92%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 18 cost 13s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 19 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.001905\n",
      "[  480/2799 ( 17%)]\tLoss: 0.000292\n",
      "[  800/2799 ( 28%)]\tLoss: 0.066388\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.000622\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.000545\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.008900\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.000856\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.000557\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.002716\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.4664,Accuracy: 737/800  \u001B[32m(92%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.3480,Accuracy: 371/401  \u001B[32m(93%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 19 cost 14s in total\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 20 \u001B[0m\n",
      "[  160/2799 (  6%)]\tLoss: 0.000674\n",
      "[  480/2799 ( 17%)]\tLoss: 0.000045\n",
      "[  800/2799 ( 28%)]\tLoss: 0.001043\n",
      "[ 1120/2799 ( 40%)]\tLoss: 0.002270\n",
      "[ 1440/2799 ( 51%)]\tLoss: 0.000121\n",
      "[ 1760/2799 ( 62%)]\tLoss: 0.004189\n",
      "[ 2080/2799 ( 74%)]\tLoss: 0.000084\n",
      "[ 2400/2799 ( 85%)]\tLoss: 0.000476\n",
      "[ 2720/2799 ( 97%)]\tLoss: 0.000183\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss: 0.5223,Accuracy: 729/800  \u001B[32m(91%)\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[32m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss: 0.3831,Accuracy: 374/401  \u001B[32m(93%)\n",
      "\u001B[0m\n",
      "\u001B[34mEpoch 20 cost 13s in total\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    start_time = time.time()\n",
    "    train(epoch)\n",
    "    test()\n",
    "    end_time = time.time()\n",
    "    print(colored(f\"Epoch {epoch} cost {end_time - start_time:.0f}s in total\", \"blue\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "filename = f'Classification_VGG_1222.pth'\n",
    "torch.save(model.state_dict(), filename)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model checkpoint\n",
    "model.load_state_dict(torch.load('Classification_VGG_1222.pth'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93       184\n",
      "           1       0.93      0.95      0.94       217\n",
      "\n",
      "    accuracy                           0.93       401\n",
      "   macro avg       0.93      0.93      0.93       401\n",
      "weighted avg       0.93      0.93      0.93       401\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 800x600 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        y_true.extend(target.cpu().numpy())\n",
    "        y_pred.extend(pred.cpu().numpy())\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# pylot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "plt.colorbar()\n",
    "\n",
    "# Adding labels for the cells\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(np.arange(len(np.unique(y_true))))\n",
    "plt.yticks(np.arange(len(np.unique(y_true))))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Release GPU memory cache\n",
    "torch.cuda.empty_cache()\n",
    "del model\n",
    "del train_loader\n",
    "del val_loader\n",
    "del test_loader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regression Task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "all_label = []\n",
    "# Parse txt label file to np array\n",
    "with open('Dataset/labels.txt') as f:\n",
    "    for line in f:\n",
    "        # Assuming each line is comma-separated\n",
    "        processed_line = [float(value) for value in line.strip().split(' ')]\n",
    "\n",
    "        # Step 4: Append processed data to list\n",
    "        all_label.append(processed_line)\n",
    "\n",
    "data_array = np.array(all_label)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split data into train, val, test\n",
    "def split_and_copy(start, end):\n",
    "    frames = [f\"file{i:04d}.jpg\" for i in range(start, end)]\n",
    "    train = frames[:int(len(frames) * 0.7)]\n",
    "    val = frames[int(len(frames) * 0.7):int(len(frames) * 0.9)]\n",
    "    test = frames[int(len(frames) * 0.9):]\n",
    "\n",
    "    for frame in train:\n",
    "        shutil.copy(os.path.join(base_dir, \"files\", frame),\n",
    "                    os.path.join(base_dir, \"reg_train\", frame))\n",
    "    for frame in val:\n",
    "        shutil.copy(os.path.join(base_dir, \"files\", frame),\n",
    "                    os.path.join(base_dir, \"reg_val\", frame))\n",
    "    for frame in test:\n",
    "        shutil.copy(os.path.join(base_dir, \"files\", frame),\n",
    "                    os.path.join(base_dir, \"reg_test\", frame))\n",
    "\n",
    "\n",
    "# 对于每个类别执行拆分和复制\n",
    "split_and_copy(1, 4001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_train: 2800\n",
      "reg_val: 800\n",
      "reg_test: 400\n"
     ]
    }
   ],
   "source": [
    "# 定义路径变量\n",
    "base_dir = \"Dataset\"\n",
    "phases = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# 创建目录结构\n",
    "for phase in phases:\n",
    "    os.makedirs(os.path.join(base_dir, f\"reg_{phase}\"), exist_ok=True)\n",
    "\n",
    "# check the number of images in each folder\n",
    "for phase in phases:\n",
    "    print(f\"reg_{phase}: {len(os.listdir(os.path.join(base_dir, f'reg_{phase}')))}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# 定义自定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    array = data_array\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        # 检查root_dir是否是目录\n",
    "        if os.path.isdir(root_dir):\n",
    "            # 遍历root_dir中的文件\n",
    "            for file in sorted(os.listdir(root_dir)):\n",
    "                if file.startswith(\"file\"):\n",
    "                    file_path = os.path.join(root_dir, file)  # 构建文件路径\n",
    "                    self.images.append(file_path)\n",
    "                    # 从文件名中提取索引并调整以匹配data_array\n",
    "                    file_index = int(file[4:8]) - 1\n",
    "                    self.labels.append(self.array[file_index, 1:].astype(np.float32))\n",
    "        else:\n",
    "            raise RuntimeError(f\"The provided root directory {root_dir} does not exist or is not a directory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "\n",
    "# 定义图像转换\n",
    "transform_TRAIN = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 创建数据集\n",
    "train_dataset = CustomDataset(root_dir='Dataset/reg_train', transform=transform_TRAIN)\n",
    "val_dataset = CustomDataset(root_dir='Dataset/reg_val', transform=transform)\n",
    "test_dataset = CustomDataset(root_dir='Dataset/reg_test', transform=transform)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# VGG16网络模型\n",
    "class Vgg16_reg_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vgg16_reg_net, self).__init__()\n",
    "\n",
    "        # 第一层卷积层\n",
    "        self.layer1 = nn.Sequential(\n",
    "            # 输入3通道图像，输出64通道特征图，卷积核大小3x3，步长1，填充1\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            # 对64通道特征图进行Batch Normalization\n",
    "            nn.BatchNorm2d(64),\n",
    "            # 对64通道特征图进行ReLU激活函数\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 输入64通道特征图，输出64通道特征图，卷积核大小3x3，步长1，填充1\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            # 对64通道特征图进行Batch Normalization\n",
    "            nn.BatchNorm2d(64),\n",
    "            # 对64通道特征图进行ReLU激活函数\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # 进行2x2的最大池化操作，步长为2\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # 第二层卷积层\n",
    "        self.layer2 = nn.Sequential(\n",
    "            # 输入64通道特征图，输出128通道特征图，卷积核大小3x3，步长1，填充1\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            # 对128通道特征图进行Batch Normalization\n",
    "            nn.BatchNorm2d(128),\n",
    "            # 对128通道特征图进行ReLU激活函数\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # 输入128通道特征图，输出128通道特征图，卷积核大小3x3，步长1，填充1\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            # 对128通道特征图进行Batch Normalization\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 进行2x2的最大池化操作，步长为2\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        # 第三层卷积层\n",
    "        self.layer3 = nn.Sequential(\n",
    "            # 输入为128通道，输出为256通道，卷积核大小为33，步长为1，填充大小为1\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            # 批归一化\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            self.layer1,\n",
    "            self.layer2,\n",
    "            self.layer3,\n",
    "            self.layer4,\n",
    "            self.layer5\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * 4 * 4, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(256, 3)  # 输出为3，对应三个回归目标\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)  # 将卷积层输出的多维数据压平成一维\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           1,792\n",
      "            Conv2d-2         [-1, 64, 128, 128]           1,792\n",
      "       BatchNorm2d-3         [-1, 64, 128, 128]             128\n",
      "       BatchNorm2d-4         [-1, 64, 128, 128]             128\n",
      "              ReLU-5         [-1, 64, 128, 128]               0\n",
      "              ReLU-6         [-1, 64, 128, 128]               0\n",
      "            Conv2d-7         [-1, 64, 128, 128]          36,928\n",
      "            Conv2d-8         [-1, 64, 128, 128]          36,928\n",
      "       BatchNorm2d-9         [-1, 64, 128, 128]             128\n",
      "      BatchNorm2d-10         [-1, 64, 128, 128]             128\n",
      "             ReLU-11         [-1, 64, 128, 128]               0\n",
      "             ReLU-12         [-1, 64, 128, 128]               0\n",
      "        MaxPool2d-13           [-1, 64, 64, 64]               0\n",
      "        MaxPool2d-14           [-1, 64, 64, 64]               0\n",
      "           Conv2d-15          [-1, 128, 64, 64]          73,856\n",
      "           Conv2d-16          [-1, 128, 64, 64]          73,856\n",
      "      BatchNorm2d-17          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-18          [-1, 128, 64, 64]             256\n",
      "             ReLU-19          [-1, 128, 64, 64]               0\n",
      "             ReLU-20          [-1, 128, 64, 64]               0\n",
      "           Conv2d-21          [-1, 128, 64, 64]         147,584\n",
      "           Conv2d-22          [-1, 128, 64, 64]         147,584\n",
      "      BatchNorm2d-23          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-24          [-1, 128, 64, 64]             256\n",
      "             ReLU-25          [-1, 128, 64, 64]               0\n",
      "             ReLU-26          [-1, 128, 64, 64]               0\n",
      "        MaxPool2d-27          [-1, 128, 32, 32]               0\n",
      "        MaxPool2d-28          [-1, 128, 32, 32]               0\n",
      "           Conv2d-29          [-1, 256, 32, 32]         295,168\n",
      "           Conv2d-30          [-1, 256, 32, 32]         295,168\n",
      "      BatchNorm2d-31          [-1, 256, 32, 32]             512\n",
      "      BatchNorm2d-32          [-1, 256, 32, 32]             512\n",
      "             ReLU-33          [-1, 256, 32, 32]               0\n",
      "             ReLU-34          [-1, 256, 32, 32]               0\n",
      "           Conv2d-35          [-1, 256, 32, 32]         590,080\n",
      "           Conv2d-36          [-1, 256, 32, 32]         590,080\n",
      "      BatchNorm2d-37          [-1, 256, 32, 32]             512\n",
      "      BatchNorm2d-38          [-1, 256, 32, 32]             512\n",
      "             ReLU-39          [-1, 256, 32, 32]               0\n",
      "             ReLU-40          [-1, 256, 32, 32]               0\n",
      "           Conv2d-41          [-1, 256, 32, 32]         590,080\n",
      "           Conv2d-42          [-1, 256, 32, 32]         590,080\n",
      "      BatchNorm2d-43          [-1, 256, 32, 32]             512\n",
      "      BatchNorm2d-44          [-1, 256, 32, 32]             512\n",
      "             ReLU-45          [-1, 256, 32, 32]               0\n",
      "             ReLU-46          [-1, 256, 32, 32]               0\n",
      "        MaxPool2d-47          [-1, 256, 16, 16]               0\n",
      "        MaxPool2d-48          [-1, 256, 16, 16]               0\n",
      "           Conv2d-49          [-1, 512, 16, 16]       1,180,160\n",
      "           Conv2d-50          [-1, 512, 16, 16]       1,180,160\n",
      "      BatchNorm2d-51          [-1, 512, 16, 16]           1,024\n",
      "      BatchNorm2d-52          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-53          [-1, 512, 16, 16]               0\n",
      "             ReLU-54          [-1, 512, 16, 16]               0\n",
      "           Conv2d-55          [-1, 512, 16, 16]       2,359,808\n",
      "           Conv2d-56          [-1, 512, 16, 16]       2,359,808\n",
      "      BatchNorm2d-57          [-1, 512, 16, 16]           1,024\n",
      "      BatchNorm2d-58          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-59          [-1, 512, 16, 16]               0\n",
      "             ReLU-60          [-1, 512, 16, 16]               0\n",
      "           Conv2d-61          [-1, 512, 16, 16]       2,359,808\n",
      "           Conv2d-62          [-1, 512, 16, 16]       2,359,808\n",
      "      BatchNorm2d-63          [-1, 512, 16, 16]           1,024\n",
      "      BatchNorm2d-64          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-65          [-1, 512, 16, 16]               0\n",
      "             ReLU-66          [-1, 512, 16, 16]               0\n",
      "        MaxPool2d-67            [-1, 512, 8, 8]               0\n",
      "        MaxPool2d-68            [-1, 512, 8, 8]               0\n",
      "           Conv2d-69            [-1, 512, 8, 8]       2,359,808\n",
      "           Conv2d-70            [-1, 512, 8, 8]       2,359,808\n",
      "      BatchNorm2d-71            [-1, 512, 8, 8]           1,024\n",
      "      BatchNorm2d-72            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-73            [-1, 512, 8, 8]               0\n",
      "             ReLU-74            [-1, 512, 8, 8]               0\n",
      "           Conv2d-75            [-1, 512, 8, 8]       2,359,808\n",
      "           Conv2d-76            [-1, 512, 8, 8]       2,359,808\n",
      "      BatchNorm2d-77            [-1, 512, 8, 8]           1,024\n",
      "      BatchNorm2d-78            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-79            [-1, 512, 8, 8]               0\n",
      "             ReLU-80            [-1, 512, 8, 8]               0\n",
      "           Conv2d-81            [-1, 512, 8, 8]       2,359,808\n",
      "           Conv2d-82            [-1, 512, 8, 8]       2,359,808\n",
      "      BatchNorm2d-83            [-1, 512, 8, 8]           1,024\n",
      "      BatchNorm2d-84            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-85            [-1, 512, 8, 8]               0\n",
      "             ReLU-86            [-1, 512, 8, 8]               0\n",
      "        MaxPool2d-87            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-88            [-1, 512, 4, 4]               0\n",
      "           Linear-89                  [-1, 512]       4,194,816\n",
      "             ReLU-90                  [-1, 512]               0\n",
      "          Dropout-91                  [-1, 512]               0\n",
      "           Linear-92                  [-1, 256]         131,328\n",
      "             ReLU-93                  [-1, 256]               0\n",
      "          Dropout-94                  [-1, 256]               0\n",
      "           Linear-95                    [-1, 3]             771\n",
      "================================================================\n",
      "Total params: 33,773,187\n",
      "Trainable params: 33,773,187\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 210.14\n",
      "Params size (MB): 128.83\n",
      "Estimated Total Size (MB): 339.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 创建模型实例\n",
    "model_reg = Vgg16_reg_net()\n",
    "# Move the model to the GPU\n",
    "model_reg = model_reg.to(device)\n",
    "\n",
    "summary(model_reg, input_size=(3, 128, 128))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "loss_object = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model_reg.parameters(), lr=0.01, momentum=0.8, weight_decay=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5, last_epoch=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    model_reg.train()  # 设置模型为训练模式\n",
    "    train_loss = 0\n",
    "    print(colored(\"Train Epoch:\", \"red\"), colored(f\" {epoch} \", \"blue\"), )\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model_reg(data)\n",
    "        loss = loss_object(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % 20 == 15:\n",
    "            print(\n",
    "                f\"[{batch_idx * len(data):5d}/{len(train_loader.dataset)} \"\n",
    "                f\"({100. * batch_idx / len(train_loader):3.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "    # 在每个 epoch 后进行验证\n",
    "    model_reg.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model_reg(data)\n",
    "            val_loss += loss_object(output, target).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(\n",
    "        colored(\"\\nValidation\", \"blue\"),\n",
    "        f'Average loss: ',\n",
    "        colored(f\"{val_loss:.4f}\", \"green\")\n",
    "    )\n",
    "\n",
    "    # 更新学习率调度器\n",
    "    print(colored(\"Learning rate:\", \"blue\"), colored(f\"{scheduler.get_last_lr()[0]:.6f}\", \"yellow\"))\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "# Test\n",
    "def test():\n",
    "    model_reg.eval()  # 设置模型为评估模式\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():  # 在评估模式下，不计算梯度\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model_reg(data)\n",
    "            test_loss += loss_object(output, target).item()  # 累积损失\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    print(\n",
    "        colored(\"Test set\", \"yellow\"),\n",
    "        f'Average loss: ',\n",
    "        colored(f\"{test_loss:.4f}\\n\", \"green\")\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 1 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.044097\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.025241\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.019795\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.038683\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0315\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.010000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 2 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.022849\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.027592\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.035015\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.036572\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0314\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.010000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 3 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.027909\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.019654\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.028420\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.029372\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0315\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.010000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 4 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.021705\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.032550\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.026641\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.028123\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0316\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.010000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0382\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 5 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.030746\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.026489\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.024355\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.039429\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0315\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.010000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0381\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 6 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.022847\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.024125\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.042755\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.019571\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0315\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.010000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 7 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.030634\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.015809\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.034140\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.022961\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0316\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.010000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0383\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 8 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.033976\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.037184\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.026273\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.025667\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.010000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 9 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.031391\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.027717\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.029419\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.024834\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0315\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.010000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 10 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.020373\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.025862\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.027699\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.023358\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0315\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.010000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 11 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.028531\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.021083\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.023357\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.037291\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0315\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.005000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0381\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 12 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.028417\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.032015\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.027235\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.025479\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0314\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.005000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 13 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.027772\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.028013\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.040449\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.029299\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.005000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 14 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.031215\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.020877\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.017513\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.023662\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0314\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.005000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 15 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.039095\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.019735\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.026557\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.027079\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0314\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.005000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 16 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.020843\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.040412\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.023376\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.026704\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0315\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.005000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 17 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.017665\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.028686\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.026090\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.035717\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0314\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.005000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 18 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.033408\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.020905\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.028033\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.032181\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0314\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.005000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 19 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.029911\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.023018\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.031816\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.043860\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0315\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.005000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 20 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.015963\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.030267\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.037585\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.021677\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0314\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.005000\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0378\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 21 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.023061\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.020115\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.031955\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.016606\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0314\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 22 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.032831\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.019801\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.020826\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.027651\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0314\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 23 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.023838\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.029282\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.027128\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.038161\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0315\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 24 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.030612\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.024327\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.019000\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.032941\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0315\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 25 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.027909\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.038550\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.019627\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.027640\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0314\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 26 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.027473\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.020760\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.029663\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.030742\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0314\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 27 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.025197\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.025722\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.036070\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.026127\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0378\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 28 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.030769\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.028175\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.023357\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.026574\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 29 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.022736\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.023694\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.038066\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.036319\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 30 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.040859\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.021015\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.031240\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.033435\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.002500\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 31 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.026218\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.025848\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.031218\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.033400\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 32 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.026176\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.027110\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.015866\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.022760\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0314\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 33 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.034183\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.032178\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.033425\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.032909\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 34 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.036760\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.025183\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.021769\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.023649\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 35 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.041314\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.025825\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.036014\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.046115\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 36 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.020081\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.032531\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.036118\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.027817\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 37 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.020336\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.043957\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.023118\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.028412\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 38 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.022452\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.022133\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.032090\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.024730\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 39 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.029797\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.027521\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.043469\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.024452\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 40 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.018262\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.028820\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.035905\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.037764\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0312\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.001250\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 41 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.019074\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.030004\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.041867\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.027407\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0312\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.000625\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 42 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.020680\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.028464\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.027879\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.030496\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.000625\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 43 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.027748\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.024787\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.035201\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.031741\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.000625\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 44 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.023402\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.048271\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.025631\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.024838\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.000625\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 45 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.025539\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.031420\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.036030\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.025180\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.000625\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 46 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.025196\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.041704\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.034978\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.029671\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.000625\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 47 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.019308\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.024276\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.020496\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.029290\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0312\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.000625\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 48 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.031913\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.029271\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.038637\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.033035\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0312\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.000625\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 49 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.037640\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.032184\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.026274\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.019521\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0313\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.000625\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0380\n",
      "\u001B[0m\n",
      "\u001B[31mTrain Epoch:\u001B[0m \u001B[34m 50 \u001B[0m\n",
      "[  480/2800 ( 17%)]\tLoss: 0.025698\n",
      "[ 1120/2800 ( 40%)]\tLoss: 0.027967\n",
      "[ 1760/2800 ( 62%)]\tLoss: 0.031224\n",
      "[ 2400/2800 ( 85%)]\tLoss: 0.021343\n",
      "\u001B[34m\n",
      "Validation\u001B[0m Average loss:  \u001B[32m0.0312\u001B[0m\n",
      "\u001B[34mLearning rate:\u001B[0m \u001B[33m0.000625\u001B[0m\n",
      "\u001B[33mTest set\u001B[0m Average loss:  \u001B[32m0.0379\n",
      "\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "filename = f'Regression_VGG_1222.pth'\n",
    "torch.save(model_reg.state_dict(), filename)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model checkpoint\n",
    "model_reg.load_state_dict(torch.load('Regression_VGG_1222.pth'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.03759666\n",
      "MAE:  0.13289125\n",
      "R2:  -0.001080308741399157\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "pred_val = []\n",
    "true_val = []\n",
    "model_reg.eval()\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model_reg(data)\n",
    "        pred_val.extend(output.cpu().numpy())\n",
    "        true_val.extend(target.cpu().numpy())\n",
    "\n",
    "print(\"MSE: \", mean_squared_error(true_val, pred_val))\n",
    "print(\"MAE: \", mean_absolute_error(true_val, pred_val))\n",
    "print(\"R2: \", r2_score(true_val, pred_val))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9 Conclusions\n",
    "This report presented a comprehensive study utilizing a VGG neural network for two distinct tasks: image classification based on smiles and 3D posture estimation from images.\n",
    "The classification task, focused on detecting smiles in the genki4k dataset, highlighted the network's capability to handle variations in facial expressions across diverse demographics. This underlines the potential of deep learning in nuanced facial expression recognition.\n",
    "Conversely, the regression task pushed the boundaries of the VGG network's application, adapting it to understand and estimate 3D human postures from 2D images. This innovative application demonstrates the versatility of neural networks in comprehending complex spatial information, even with limited training data.\n",
    "\n",
    "**Proposed Improvement: Unified Multi-Task Learning Model**\n",
    "\n",
    "Current Approach\n",
    "In the initial phase of our project, we employed two distinct models based on the VGG neural network architecture. The first model was dedicated to the classification task of detecting smiles in images, utilizing the genki4k dataset. The second model focused on the regression task of estimating 3D head postures from the same set of images. While effective, this approach necessitated separate training, tuning, and maintenance for each model, leading to increased computational resources and time.\n",
    "\n",
    "Proposed Improvement\n",
    "In a bid to enhance efficiency and streamline our process, we propose a significant improvement to our methodology: the integration of both tasks - smile detection and head posture estimation - into a single unified model. This approach leverages the concept of multi-task learning, where a shared neural network learns to perform multiple tasks simultaneously.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
